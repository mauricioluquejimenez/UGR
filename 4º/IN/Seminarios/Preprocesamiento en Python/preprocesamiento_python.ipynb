{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repaso: Manejo de Pandas\n",
    "\n",
    "## Selección de datos\n",
    "\n",
    "Antes de nada vamos a recordar cómo seleccionar atributos e instancias.\n",
    "\n",
    "Cargamos pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import ProxyBasicAuthHandler\n",
    "import pandas as pd\n",
    "from pandas.core.common import random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sw_characters.csv\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos consultar los atributos con:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de atributos\n",
    "\n",
    "Sobre este conjunto de datos haremos las siguientes operaciones de\n",
    "selección (en todas ellas el resultado es un nuevo conjunto de datos):\n",
    "\n",
    "- selección de atributos concretos.\n",
    "- selección de instancias concretas.\n",
    "\n",
    "\n",
    "1. selección de 3 variables en concreto: **name**, **height**, **gender**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se seleccionan algunas caracteristicas: name, height\n",
    "# and gender\n",
    "data1 = df[[\"name\", \"height\", \"gender\"]]\n",
    "print(data1.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. todas las variables excepto las indicadas a continuación:\n",
    "**birth_year** y **gender**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se mantienen todas las variables exceptuando\n",
    "# birth_year y gender\n",
    "df2 = df.drop(['birth_year', 'gender'], axis=1)\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de instancias\n",
    "\n",
    "Es fácil filtrar un valor numérico o por valor exacto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[data1[\"gender\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se combina con & y | (no dobles) usando paréntesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = (data1[\"gender\"].isnull()) & (data1[\"height\"] < 100)\n",
    "data1[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es más difícil si queremos filtrar según uno o varios valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.skin_color.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por varios valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.skin_color.isin([\"blonde\", \"brown\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[(~df2.skin_color.isnull()) &\n",
    "    (df2.skin_color.str.contains(\"blo*\", case=True,regex=True))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renombrado de variables\n",
    "\n",
    "Se puede hacer directamente editando columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df2.copy()\n",
    "df_tmp.columns = [\"V1\",\"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\"]\n",
    "df_tmp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero lo suyo es renombrar usando un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={\"skin_color\": \"color_piel\"}, inplace=True)\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conocer los tipos originales del dataset\n",
    "\n",
    "`info` nos devuelve los tipos (`object` son `string`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conocer información sobre los valores numéricos se puede hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y más en detalle se puede usar `describe` con un atributo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver las frecuencias se puede usar `values_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede normalizar (y no ordenar si se quiere):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.value_counts(normalize=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`value_counts()` también permite medir frecuencia de combinaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"species\", \"hair_color\"]].value_counts(normalize=True).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"species\", \"hair_color\"]].value_counts(normalize=False, ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valores perdidos\n",
    "\n",
    "## Valores perdidos\n",
    "\n",
    "Un problema habitual suele consistir en la presencia de datos datos.\n",
    "\n",
    "Es importante tener claro cómo leer los datos indicando la posible ausencia de\n",
    "valor, usando *na_values*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"nulos.csv\", na_values=['?', '', 'NA'])\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay múltiples técnicas para tratar los datos perdidos. Es importante valorar si la\n",
    "técnica de aprendizaje es capaz de trabajar con datos perdidos o no.\n",
    "\n",
    "Para conocer los nulos (en porcentaje):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_nulos = data.isnull().sum()/data.shape[0]\n",
    "ratio_nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opción directa (eliminar nulos)\n",
    "\n",
    "Se pueden eliminar o bien los atributos que tienen demasiados nulos, o eliminar\n",
    "tuplas.\n",
    "\n",
    "Eliminar atributos que superen un umbral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()\n",
    "\n",
    "for i, atrib in enumerate(data):\n",
    "    if ratio_nulos[i] > 0.4:\n",
    "        data2.drop(atrib, axis=1, inplace=True)\n",
    "\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar todas las filas con algún nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drop = data.dropna()\n",
    "print(data_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drop = data2.dropna()\n",
    "print(data_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar valores perdidos con paquetes externos\n",
    "\n",
    "Dado que el aprendizaje en `scikit-learn` no es compatible con valores perdidos,\n",
    "vamos a probar distintas opciones que la propia librería nos permite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris_dataset = datasets.load_iris(as_frame=True)\n",
    "X_iris = iris_dataset.data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar los métodos añadidos nulos al dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the dataset to test sk-learn imputation values tools\n",
    "np.random.seed(42)\n",
    "rows = np.random.randint(0, np.shape(X_iris)[0], 50)\n",
    "# No modifico la última característica\n",
    "cols = np.random.randint(0, np.shape(X_iris)[1]-1, 50)\n",
    "X_iris_missing = X_iris.to_numpy()\n",
    "#Add missing values in random entries from the iris dataset\n",
    "X_iris_missing[rows, cols] = np.NaN\n",
    "X_iris_missing = pd.DataFrame(X_iris_missing, columns=X_iris.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos ahora nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_iris_missing.iloc[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputación Univariante\n",
    "\n",
    "Los objetos de tipo *Impute* permite reemplazar los valores nulos. Para ello\n",
    "pueden usar un valor constante o una estadística (media, mediana o más\n",
    "frecuente) para cada columna con nulos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# strategy puede ser \"mean\", \"median\", \"most_frequent\", \"constant\".\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed_X = pd.DataFrame(imp.fit_transform(X_iris_missing), columns=X_iris.columns)\n",
    "print(imputed_X.iloc[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando KNN\n",
    "\n",
    "Se pueden imputar usando el algoritmo de K vecinos (KNN). Para cada atributo\n",
    "perdido se calcula a partir de los K vecinos más cercanos que no sea nulo. Los\n",
    "vecinos pueden ser diferentes para cada atributo.\n",
    "\n",
    "Si no encuentra vecinos sin nulos, el atributo es borrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "Knn_imp = KNNImputer(n_neighbors=4).fit(X_iris_missing)\n",
    "imputed_X = pd.DataFrame(Knn_imp.transform(X_iris_missing), columns=X_iris.columns)\n",
    "print(imputed_X.iloc[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de entrada\n",
    "\n",
    "## Estandarización\n",
    "\n",
    "Estandarización es un requisito de muchos modelos de ML, como los basados en\n",
    "distancias.\n",
    "\n",
    "`scikit-learn` permite hacer estandarización, hay [múltiples\n",
    "opciones](]https://scikit-learn.org/stable/modules/preprocessing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a preprocessing object\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris_dataset = datasets.load_iris(as_frame=True)\n",
    "X_iris = iris_dataset.data.copy()\n",
    "scaler = StandardScaler().fit(X_iris)\n",
    "#Check the mean and the std of the training set\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado se puede aplicar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_iris_scaled = scaler.transform(X_iris)\n",
    "print(X_iris.iloc[:5,:])\n",
    "print(\"StandardScaler: \")\n",
    "print(X_iris_scaled[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the dataset using the preprocessin object and check results\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_iris), columns=X_iris.columns)\n",
    "print(X_scaled.mean(axis=0))\n",
    "print(X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig,axs = plt.subplots(2,1)\n",
    "X_iris.plot.kde(ax=axs[0])\n",
    "X_scaled.plot.kde(ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro muy común es `MinMaxScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_iris_scaled2 = MinMaxScaler().fit_transform(X_iris)\n",
    "print(X_iris.iloc[:5,:])\n",
    "print(\"MinMaxScaler: \")\n",
    "print(X_iris_scaled2[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización\n",
    "\n",
    "La normalización es escalar las muestras individuales para que tenga una normal\n",
    "unidad.\n",
    "\n",
    "Es esencial para espresiones cuadráticas, o que usen un *kernel* que mida similaridad de\n",
    "pares de instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "print(X_iris.iloc[:4,:])\n",
    "X_normalized = pd.DataFrame(normalize(X_iris), columns=X_iris.columns)\n",
    "print(X_normalized.iloc[:4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atributos categóricos\n",
    "\n",
    "## Codificando atributos categóricos\n",
    "\n",
    "Es común atributos con valores categóricos. `Scikit-learn` no es capaz de\n",
    "procesarlos, por lo que es necesario transformarlo a valores numéricos.\n",
    "\n",
    "- LabelEncoder y OrdinalEncoder: Asigna un valor numérico por cada categoría.\n",
    "\n",
    "- OneHotEncoder: Codifica cada categoría usando una nueva columna.\n",
    "\n",
    "\n",
    "Los Label/OrdinalEncoder asigna un orden entre las categorías que suele ser\n",
    "'falso' si ese concepto no existe.\n",
    "\n",
    "## LabelEncoder y OrdinalEncoder\n",
    "\n",
    "Ambos asignan un valor numérico distinto a cada categoría.\n",
    "\n",
    "Diferencia:\n",
    "\n",
    "- `OrdinalEncoder` puede procesar varias columnas, se usa para características.\n",
    "\n",
    "- `LabelEncoder` solo procesa un elemento, se usa para el atributo objetivo\n",
    "  (*target*).\n",
    "\n",
    "\n",
    "Advertencia: Evitar hacer esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "targets_train = [\"rubio\", \"moreno\", \"pelirrojo\", \"azul\"]\n",
    "targets_test = [\"moreno\", \"pelirrojo\"]\n",
    "\n",
    "targets_train_num = LabelEncoder().fit_transform(targets_train)\n",
    "targets_test_num = LabelEncoder().fit_transform(targets_test)\n",
    "print(targets_train)\n",
    "print(targets_train_num)\n",
    "print(targets_test)\n",
    "print(targets_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas no coinciden.\n",
    "\n",
    "\n",
    "Para evitarlo hay que hacer *fit* solo con el de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler_target = LabelEncoder()\n",
    "targets_train_num = labeler_target.fit_transform(targets_train)\n",
    "targets_test_num = labeler_target.transform(targets_test)\n",
    "print(targets_train)\n",
    "print(targets_train_num)\n",
    "print(targets_test)\n",
    "print(targets_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar siempre los labeler (diccionario por nombre, ...).\n",
    "\n",
    "\n",
    "Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_df = pd.DataFrame({'age': [30, 41, 42, 21],\n",
    "                         'pelo': targets_train,\n",
    "                         'ojos': ['azules', 'verdes', 'marrones', 'marrones']})\n",
    "data_test_df = pd.DataFrame({'age': [25, 23],\n",
    "                             'pelo': targets_test,\n",
    "                             'ojos': ['verdes', 'azules']})\n",
    "print(data_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplicar el etiquetado.\n",
    "\n",
    "\n",
    "- Opción 1: Sólo con Label Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelers = {}\n",
    "cols = {}\n",
    "atribs = [\"pelo\", \"ojos\"]\n",
    "data_train_num = data_train_df.copy()\n",
    "data_test_num = data_test_df.copy()\n",
    "\n",
    "for i in atribs:\n",
    "    cols[i] = LabelEncoder()\n",
    "    data_train_num[i] = cols[i].fit_transform(data_train_num[i])\n",
    "    data_test_num[i] = cols[i].transform(data_test_num[i])\n",
    "\n",
    "print(data_train_num)\n",
    "print(data_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Opción 2: Usando `OrdinalEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "atribs = [\"pelo\", \"ojos\"]\n",
    "labelers = OrdinalEncoder(dtype=np.int32) # Por defecto usa float\n",
    "data_train_num = data_train_df.copy()\n",
    "data_test_num = data_test_df.copy()\n",
    "\n",
    "data_train_num[atribs] = labelers.fit_transform(data_train_df[atribs])\n",
    "data_test_num[atribs] = labelers.transform(data_test_df[atribs])\n",
    "\n",
    "print(data_train_num)\n",
    "print(data_test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *LabelEncoder*, *OrdinalEncoder* y orden\n",
    "\n",
    "Esta codificación está considerando un orden entre categorías.\n",
    "\n",
    "En algunos casos como ['pequeño', 'mediano', 'grande'] puede tener sentido pero\n",
    "la mayoría de las veces no.\n",
    "\n",
    "Cuando no (como 'pelo' o 'color' del ejemplo anterior) es necesario aplicar\n",
    "*OneHotEncoder*.\n",
    "\n",
    "\n",
    "`OneHotEncoder` crea una columna por categoría (ej: 'azul') indicando si se cumple\n",
    "o no.\n",
    "\n",
    "- Aumenta el número de columnas.\n",
    "\n",
    "- Evita suponer un orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Por defecto es matriz sparse\n",
    "encoder = OneHotEncoder(sparse=False, dtype=np.int32)\n",
    "data_train_hot = encoder.fit_transform(data_train_df[atribs])\n",
    "print(data_train_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede convertir a *dataframe*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = encoder.get_feature_names_out()\n",
    "print(new_columns)\n",
    "data_train_hot = pd.DataFrame(data_train_hot, columns=new_columns)\n",
    "# Copio el resto de atributos\n",
    "data_train_hot['age'] = data_train_df['age']\n",
    "print(data_train_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies en Pandas\n",
    "\n",
    "Pandas ya soporta el *hotencoding*, pero presenta problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data_train_df[['pelo', 'ojos']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomiendo usar `OneHotEncoder` por tener más opciones.\n",
    "\n",
    "## Valores binarios\n",
    "\n",
    "Si el valor numérico es binario, ej: vivo/muerto no es necesario aplicar el\n",
    "*hotencoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Employed\", \"Place\", 'Browser']\n",
    "X = [['employed', 'from US', 'uses Safari'], ['unemployed', 'from Europe', 'uses Firefox'], ['unemployed', 'from Asia', 'uses Chrome']]\n",
    "enc = OneHotEncoder(drop='if_binary')\n",
    "trans_X = enc.fit_transform(X)\n",
    "transformed_X = pd.DataFrame(trans_X.toarray(), columns=enc.get_feature_names_out())\n",
    "print(transformed_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines y ColumnTransformer\n",
    "\n",
    "## Aplicar `OneHotEncoder`\n",
    "\n",
    "Scikit-learn permite combinar transformaciones con `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['pelo', 'ojos']),\n",
    "  remainder='passthrough') # Para ignorar el resto y no dar error\n",
    "\n",
    "transformed = transformer.fit_transform(data_train_df)\n",
    "data_train_num = pd.DataFrame(transformed, columns=transformer.get_feature_names_out())\n",
    "print(data_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ColumnTransformer` permite procesar distintos datasets.\n",
    "\n",
    "En conjunción con make_column_selector (que permite filtrar atributos por su tipo) es muy potente y cómodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],\n",
    "                  'rating': [5, 3, 4, 5]})\n",
    "ct = make_column_transformer(\n",
    "      (StandardScaler(),\n",
    "       make_column_selector(dtype_include=np.number)),  # rating\n",
    "      (OneHotEncoder(),\n",
    "       make_column_selector(dtype_include=object)))  # city\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Facilitan aplicar distintos preprocesamientos.\n",
    "\n",
    "Un *pipeline* se compone de una serie de transformaciones que van sufriendo el\n",
    "dataset (se puede incluir el modelo a aprender).\n",
    "\n",
    "Un *pipeline* se usa igual que un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "iris_targets = iris_dataset.target\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, iris_targets,\n",
    "                                                    random_state=0)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se puede aplicar con validación cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "scores = cross_val_score(pipe, X_iris, iris_targets, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por comodidad se puede usar `make_pipeline` con tantos atributos como\n",
    "procesamientos y/o mdelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = make_pipeline(StandardScaler(), SVC(C=1))\n",
    "cross_val_score(model, X_iris, iris_targets, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines y ColumnTransformer\n",
    "\n",
    "También se pueden combinar con `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "trans = make_column_transformer(\n",
    "    (StandardScaler(), [\"age\"]),\n",
    "    (OneHotEncoder(), [\"pelo\", \"ojos\"])\n",
    "    )\n",
    "trans.fit_transform(data_train_df)\n",
    "pipe = make_pipeline(trans, SVC())\n",
    "print(data_train_df)\n",
    "pipe.fit(data_train_df, [0, 0, 1, 1])\n",
    "print(data_test_df)\n",
    "pipe.predict(data_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretización\n",
    "\n",
    "## Discretización usando rangos\n",
    "\n",
    "A menudo no nos interesa un valor numéricos (ej: `age`) sino convertirlo en un conjunto discreto de valores (*joven*, *adulto*, *mayor*).\n",
    "\n",
    "La clase `K-bins` permite discretizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "#Build a discretizer object indicating three bins for every feature\n",
    "est = KBinsDiscretizer(n_bins=[3, 3, 3, 3], encode='ordinal').fit(X_iris)\n",
    "#Check feature maximum and minimum values \n",
    "# print(np.max(X_iris, axis = 0))\n",
    "# print(np.min(X_iris, axis = 0))\n",
    "#Check binning intervals\n",
    "print(est.bin_edges_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print discretization results\n",
    "print(X_iris.iloc[:5,])\n",
    "discretized_X = pd.DataFrame(est.transform(X_iris), columns=X_iris.columns)\n",
    "print(discretized_X.iloc[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distintas estrategias de discretización\n",
    "\n",
    "El criterio de discretización puede ser cambiado con el parámetro `strategy`.\n",
    "\n",
    "Una tendencia común sería una uniforme:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "age_disc = est.fit_transform(data_train_df[['age']])\n",
    "print(est.bin_edges_)\n",
    "print(age_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No todos los rangos tienen interés, pueden concentrarse.\n",
    "\n",
    "\n",
    "A menudo la mejor estrategia depende de la frecuencia (comportamiento por defecto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "age_disc = est.fit_transform(data_train_df[['age']])\n",
    "print(est.bin_edges_)\n",
    "print(age_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera, discretiza más en detalle los intervalos más comunes.\n",
    "\n",
    "\n",
    "La otra opción es la estrategia `kmean` que aplica una clasificación `kmeans` sobre cada algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a discretizer object indicating three bins for every feature and using the kmeans strategy\n",
    "est = KBinsDiscretizer(n_bins=[3, 3, 3, 3], encode='ordinal', strategy='kmeans').fit(X_iris)\n",
    "#Check binning intervals and results\n",
    "print(est.bin_edges_)\n",
    "discretized_X = pd.DataFrame(est.transform(X_iris), columns=X_iris.columns)\n",
    "print(discretized_X.iloc[:5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad\n",
    "\n",
    "## Reducción de dimensionalidad\n",
    "\n",
    "Una de las prácticas más comunes de procesamiento de datos es la reducción de la\n",
    "dimensional, lo cual ayuda a transformar o seleccionar las características que mejor\n",
    "representan la estructura, y que por tanto, son más adecuadas para el\n",
    "aprendizaje.\n",
    "\n",
    "## Reducción de la Dimensionalidad No supervisada\n",
    "\n",
    "Si el número de características es alto, puede ser útil `reducirlas` mediante\n",
    "una fase no supervisada.\n",
    "\n",
    "### Principal Components Analysis **(PCA)**\n",
    "\n",
    "Descomponer un *dataset* multivariante en un conjunto de componentes ortogonales\n",
    "que explica la cantidad de varianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, datasets, decomposition\n",
    "\n",
    "#Load the iris dataset and scale it\n",
    "X_iris, y_iris = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_iris)\n",
    "X_scaled = scaler.transform(X_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplico el PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply principal componentes analysis to reduce the iris number of features from 4 to 3\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "print(X_scaled[:3,:])\n",
    "print(\"Con menos dimensión\")\n",
    "print(X_reduced[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizando en 2D el PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = pd.DataFrame(X_reduced, columns=[\"X1\", \"X2\", \"X3\"])\n",
    "#Plot the iris dataset in a 2D pairplot\n",
    "df[\"target\"] = iris_dataset.frame[\"target\"]\n",
    "g = sns.pairplot(df, hue=\"target\", palette=sns.color_palette(\"hls\", 3),\n",
    "                 height=1.25, aspect=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de Características\n",
    "\n",
    "## Selección de Características\n",
    "\n",
    "En este caso se reducen las características eligiendo las características que\n",
    "permitirían un mejor desempeño del clasificador.\n",
    "\n",
    "## Selección de Características Secuencial\n",
    "\n",
    "La selección secuencial (*Forward-SFS*) busca iterativamente una nueva\n",
    "característica a añadir a las ya seleccionadas. Empieza con cero características\n",
    "y escoge aquella que maximiza aplicando CV usando un estimador (cualquiera le\n",
    "vale, pero mejor que no sea lento) sobre una única característica.\n",
    "\n",
    "Luego repite el procedimiento añadiendo una nueva característica cada vez, hasta\n",
    "terminar con el número pedido de características.\n",
    "\n",
    "## Backward-SFS\n",
    "\n",
    "*Backward-SFS* sigue la misma idea, pero al revés, en vez de ir añadiendo va\n",
    "eliminando características aplicando un estimador.\n",
    "\n",
    "No dan los mismos resultado, ni son igualmente eficientes. Si tenemos 10\n",
    "características y queremos siete será más eficiente *Backward-SFS* que\n",
    "*Forward-SFS*.\n",
    "\n",
    "`Scikit-learn` ofrece `SequentialFeatureSelector` que implementa ambos (\n",
    "direction puede ser *forward* o *backward*).\n",
    "\n",
    "\n",
    "Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "#Perform FORDWARD feature selection over the diabetes dataset to reduce it to 3 dimensions\n",
    "sfs_forward = SequentialFeatureSelector(clf, n_features_to_select=3, direction=\"forward\")\n",
    "sfs_forward_fitted = sfs_forward.fit(X_diabetes, y_diabetes)\n",
    "\n",
    "X_reduced_for = sfs_forward.transform(X_diabetes)\n",
    "print(X_reduced_for.shape)\n",
    "print(sfs_forward_fitted.get_support())\n",
    "atribs = np.array(diabetes.feature_names)\n",
    "print(\"Atributos elegidos\")\n",
    "print(atribs[sfs_forward_fitted.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando con *backward*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform BACKWARD feature selection over the diabetes dataset to reduce it to 3 dimensions\n",
    "sfs_backward = SequentialFeatureSelector(clf, n_features_to_select=3, direction=\"backward\")\n",
    "sfs_backward_fitted = sfs_backward.fit(X_diabetes, y_diabetes)\n",
    "\n",
    "X_reduced_back = sfs_backward_fitted.transform(X_diabetes)\n",
    "print(X_reduced_back.shape)\n",
    "print(sfs_backward_fitted.get_support())\n",
    "print(\"Atributos elegidos\")\n",
    "print(atribs[sfs_backward_fitted.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos anómalos\n",
    "\n",
    "## Datos anómalos\n",
    "\n",
    "A veces en los datos se presentan valores anómalos, que se han introducido, por\n",
    "ejemplo, debido a errores en los procesos de recogida de datos.\n",
    "\n",
    "Quizás el valor anómalo se deba a una cambio en la distribución de valores y no\n",
    "a un error.\n",
    "\n",
    "La intuición básica en las técnicas detección de anomalías es:\n",
    "\n",
    "- La mayoría de los datos siguen una determinada distribución.\n",
    "- Los datos las anomalías representan entonces una distribución distinta,\n",
    "que no coincide con el resto.\n",
    "\n",
    "## Detección *a mano*\n",
    "\n",
    "Un método clásico es considerar como datos anómalos aquellos para los que el\n",
    "valor de un atributo esté fuera del 1.5*rango intercuartil\n",
    "\n",
    "Supongamos unos datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train data\n",
    "rng = np.random.RandomState(42)\n",
    "X_orig = 0.3 * rng.randn(100, 2)\n",
    "#X_good = X_orig-4\n",
    "X_good = np.r_[X_orig + 2, X_orig - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = rng.uniform(low=5, high=8, size=(10, 2))\n",
    "X = np.vstack([X_good, X_outliers])\n",
    "np.random.shuffle(X)\n",
    "X_df = pd.DataFrame(X, columns=[\"V1\", \"V2\"])\n",
    "print(X_df.shape)\n",
    "print(X_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(X_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(X_df, kind=\"box\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplicar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar outliers como aquellos casos fuera de 1.25 veces el rango intercuartil\n",
    "ratio = 1.25\n",
    "Q1 = X_df.quantile(0.25)\n",
    "Q3 = X_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((X_df < (Q1 - ratio * IQR)) |(X_df > (Q3 + ratio * IQR))).any(axis=1)\n",
    "X_df_irq= X_df.copy()\n",
    "X_df_irq[\"outlier\"] = outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"V1\", y=\"V2\", data=X_df_irq, hue=\"outlier\", aspect=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección Outliers usando scikit-learn (*LocalOutlierFactor*)\n",
    "\n",
    "Esta técnica mide la desviación local de una muestra respecto a los vecinos\n",
    "(usando k-vecinos). Al comparar la distancia local con la de los vecinos, se\n",
    "ientifica las instancias con una densidad sustanciamente menor que sus vecinos.\n",
    "\n",
    "Cerca de -1 si lo considera *outlier*, 1 en caso contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors=10)\n",
    "X_df_loc = X_df.copy()\n",
    "X_df_loc[\"outlier\"] = np.abs(clf.fit_predict(X_df) - -1) <= 1e-3\n",
    "print(X_df_loc.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"V1\", y=\"V2\", data=X_df_loc, hue=\"outlier\", aspect=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
